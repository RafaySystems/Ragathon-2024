{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95706d3c-76ea-4029-8435-1664e2922e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_trt.llms import TritonTensorRTLLM\n",
    "\n",
    "# Connect to the TRT-LLM Llama-2 model running on the Triton server at the url below\n",
    "# Replace \"llm\" with the url of the system where llama2 is hosted\n",
    "triton_url = \"llm:8001\"\n",
    "pload = {\n",
    "            'tokens':500,\n",
    "            'server_url': triton_url,\n",
    "            'model_name': \"ensemble\"\n",
    "}\n",
    "llm = TritonTensorRTLLM(**pload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "760e8e51-5f09-4d00-9fb2-392dec7618bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "LLAMA_PROMPT_TEMPLATE = (\n",
    " \"<s>[INST] <<SYS>>\"\n",
    " \"Use the following context to answer the user's question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    " \"<</SYS>>\"\n",
    " \"<s>[INST] Context: {context} Question: {question} Only return the helpful answer below and nothing else. Helpful answer:[/INST]\"\n",
    ")\n",
    "\n",
    "LLAMA_PROMPT = PromptTemplate.from_template(LLAMA_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b1140-3889-4579-ac2c-f99493eb814b",
   "metadata": {},
   "source": [
    "# Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de82bdff-dd2e-44f1-bc45-332443835641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "import time\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "TEXT_SPLITTER_MODEL = \"intfloat/e5-large-v2\"\n",
    "TEXT_SPLITTER_TOKENS_PER_CHUNK = 510\n",
    "TEXT_SPLITTER_CHUNCK_OVERLAP = 200\n",
    "\n",
    "\n",
    "documents_path = \"./docs\"\n",
    "documents = list(Path(documents_path).glob(\"*.pdf\"))\n",
    "\n",
    "document_chunks = []\n",
    "for document in documents:\n",
    "    loader = UnstructuredFileLoader(document.as_posix())\n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "        model_name=TEXT_SPLITTER_MODEL,\n",
    "        tokens_per_chunk=TEXT_SPLITTER_TOKENS_PER_CHUNK,\n",
    "        chunk_overlap=TEXT_SPLITTER_CHUNCK_OVERLAP,\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    document_chunks += text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4502fca6-4c81-4ce6-9ccb-cf54c67b12c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1282.9010062217712 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Milvus\n",
    "import torch\n",
    "import time\n",
    "\n",
    "#Running the model on CPU as we want to conserve gpu memory.\n",
    "#In the production deployment (API server shown as part of the 5th notebook we run the model on GPU)\n",
    "model_name = \"intfloat/e5-large-v2\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "start_time = time.time()\n",
    "vectorstore = Milvus.from_documents(documents=document_chunks, embedding=hf_embeddings,\n",
    "                                    collection_name=\"rafay_ragathon\",\n",
    "                                    connection_args={\"host\": \"milvus\", \"port\": \"19530\"})\n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "023ff513-3a72-4cb9-a5a1-ad55894d3a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6.757351875305176 seconds ---\n",
      "  I don't have access to the specific safety evaluation of llama2 chat, as it is a proprietary document owned by Microsoft. However, I can provide some general information on the safety considerations for chatbots and conversational AI systems.\n",
      "\n",
      "Safety evaluation of chatbots and conversational AI systems involves assessing the potential risks and hazards associated with these technologies, such as:\n",
      "\n",
      "1. Data privacy and security: Chatbots and conversational AI systems often have access to sensitive user data, which can be vulnerable to cyber attacks, data breaches, or unauthorized access.\n",
      "2. User safety: Chatbots and conversational AI systems can potentially engage users in harmful or dangerous activities, such as scams, phishing attacks, or cyberbullying.\n",
      "3. Ethical considerations: Chatbots and conversational AI systems may be used to spread misinformation, propaganda, or hate speech, which can have serious ethical implications.\n",
      "4. Dependence on AI: Over-reliance on chatbots and conversational AI systems can lead to a loss of critical thinking skills, decision-making abilities, and social skills.\n",
      "5. Unintended consequences: Chatbots and conversational AI systems can have unintended consequences, such as perpetuating biases, reinforcing harmful stereotypes, or exacerbating social inequalities.\n",
      "\n",
      "To mitigate these risks, it is essential to conduct thorough safety evaluations of chatbots and conversational AI systems, including:\n",
      "\n",
      "1. Risk assessments: Identifying potential risks and hazards associated with the technology and users.\n",
      "2. Safety testing: Testing the technology with diverse user groups to identify potential safety issues.\n",
      "3. Privacy and security protocols: Implementing robust privacy and security measures to protect user data.\n",
      "4. Ethical guidelines: Establishing ethical guidelines and standards for the development and deployment of chatbots and conversational AI systems.\n",
      "5. User education: Providing users with clear guidelines and expectations for interacting with chatbots and conversational AI systems.\n",
      "\n",
      "By conducting thorough safety evaluations and implementing\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import time\n",
    "\n",
    "chain = (\n",
    "    {\"context\": vectorstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | LLAMA_PROMPT\n",
    "    | llm\n",
    ")\n",
    "start_time = time.time()\n",
    "output = \"\"\n",
    "for token in chain.stream(question):\n",
    "    output += token\n",
    "    \n",
    "print(f\"\\n--- {time.time() - start_time} seconds ---\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78822c95-051d-4e6e-8fe0-131765d9a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "\n",
    "qna_data = pd.read_csv(\"./qna_data.csv\")\n",
    "\n",
    "qna_data = qna_data[qna_data[\"Source Chunk Type\"] == \"Text\"]\n",
    "qna_data = qna_data[qna_data[\"Question Type\"] == \"Single-Doc Multi-Chunk RAG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68b8454e-0763-4845-9b73-7bec2278bf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1662/4038091912.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  question = row[0]\n",
      "/tmp/ipykernel_1662/4038091912.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  answer =  row[4]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m answer \u001b[38;5;241m=\u001b[39m  row[\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m      7\u001b[0m context \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m _docs \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[38;5;241m.\u001b[39mget_relevant_documents(question)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _doc \u001b[38;5;129;01min\u001b[39;00m _docs:\n\u001b[1;32m     11\u001b[0m     context\u001b[38;5;241m.\u001b[39mappend(_doc\u001b[38;5;241m.\u001b[39mpage_content)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "_data = []\n",
    "for idx, row in qna_data.iterrows():\n",
    "\n",
    "    question = row[0]\n",
    "    answer =  row[4]\n",
    "\n",
    "    context = []\n",
    "    _docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    for _doc in _docs:\n",
    "        context.append(_doc.page_content)\n",
    "\n",
    "    # context = \"\\n\".join(context)\n",
    "\n",
    "    start_time = time.time()\n",
    "    output = \"\"\n",
    "    for token in chain.stream(question):\n",
    "        output += token\n",
    "        \n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    _data.append([question, answer, context, output, inference_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "088be92a-619d-4abe-be78-cef5bf24352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ragas Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "681a8acf-06a7-4547-8ffe-cf38132171f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_eval \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43m_data\u001b[49m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontexts\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name '_data' is not defined"
     ]
    }
   ],
   "source": [
    "df_eval = pd.DataFrame(_data, columns=[\"question\", \"ground_truth\", \"contexts\", \"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4426894-75e0-41b5-8b1a-5c8ae6c63bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa8fd22-d0ff-4382-88bb-b47d41625603",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = Dataset.from_pandas(df_eval)\n",
    "result = evaluate(eval_dataset, metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],)\n",
    "res = result.to_pandas()[[\"context_precision\",\"faithfulness\",\"answer_relevancy\",\"context_recall\"]]\n",
    "res.fillna(0.0,inplace=True)\n",
    "\n",
    "res.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0a9af-6ab8-44c0-a22d-e601240b182b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
