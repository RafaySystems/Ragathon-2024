{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec956136-688e-4981-a799-860ca96fa20c",
   "metadata": {},
   "source": [
    "# RAG-a-Thon Sample Application\n",
    "\n",
    "A RAG-a-Thon is a hackathon focused on RAG (Retrieval-Augmented Generation) use cases. A RAG application is inherently complex due to its multifaceted nature. It requires integrating various components and infrastructure to create a cohesive application. Tuning and optimizing the performance of RAG models is essential for improving their effectiveness in real-world applications. Below is a typical flow diagram for a RAG application: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17859cb8-219d-4adf-b558-6da91ad7e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Milvus\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b1140-3889-4579-ac2c-f99493eb814b",
   "metadata": {},
   "source": [
    "## Document Processing and Chunking\n",
    "\n",
    "This code snippet is designed for extracting text from PDF documents and processing it into manageable chunks optimized for further analysis with machine learning models. Initially, it sets up an embedding model named `intfloat/e5-large-v2` and defines parameters for text chunking, including the number of tokens per chunk (100) and the overlap between consecutive chunks (20 tokens). The script searches for PDF documents in a specified directory (\"./docs\"), loading each document found.\n",
    "\n",
    "For every document, it utilizes an UnstructuredFileLoader to read the document's content and then employs a `SentenceTransformersTokenTextSplitter` configured with the embedding model and chunking parameters. This splitter divides the document's text into segments that adhere to the specified token limit and overlap criteria, facilitating efficient processing and analysis. Finally, it reports the total execution time for the entire operation, indicating the process's completion and performance efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de82bdff-dd2e-44f1-bc45-332443835641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from documents and chunking completed. Executed in 343.1030306816101 seconds\n"
     ]
    }
   ],
   "source": [
    "embedding_model_name = \"intfloat/e5-large-v2\"\n",
    "tokens_per_chunk = 100\n",
    "chunk_overlap = 20\n",
    "\n",
    "documents_path = \"./docs\"\n",
    "documents = list(Path(documents_path).glob(\"*.pdf\"))\n",
    "\n",
    "document_chunks = []\n",
    "\n",
    "start_time = time.time()\n",
    "for document in documents:\n",
    "    loader = UnstructuredFileLoader(document.as_posix())\n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "        model_name=embedding_model_name,\n",
    "        tokens_per_chunk=tokens_per_chunk,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "    document_chunks += text_splitter.split_documents(data)\n",
    "\n",
    "print(f\"Extracting data from documents and chunking completed. Executed in {time.time() - start_time} seconds\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c3d4f-7c48-4730-83b4-a71c2f1926ab",
   "metadata": {},
   "source": [
    "## Embedding Generation and Vector Store Integration\n",
    "\n",
    "This code segment is focused on generating embeddings for document chunks and integrating them into a vector storage solution for efficient similarity search and retrieval. It starts by configuring the embedding model to utilize a CUDA device for acceleration and setting embedding normalization to false. Using the HuggingFaceEmbeddings class, it initializes an embedding model with these specifications, including a dynamically specified model name.\n",
    "\n",
    "The process records the start time for performance measurement, then proceeds to generate embeddings for the previously created document chunks. It employs the Milvus class to directly load these embeddings into a vector store, creating a new collection named `rafay_ragathon_2024`. Connection parameters for the Milvus server are specified, indicating where the vector store is hosted.\n",
    "\n",
    "Finally, the script concludes by reporting the time taken to compute the embeddings and load them into the Milvus Vector Store. This setup enables sophisticated query capabilities based on the semantic similarity of the document chunks, enhancing the efficiency and effectiveness of data retrieval and analysis processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4502fca6-4c81-4ce6-9ccb-cf54c67b12c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the embeddings for each chunk and loading it to Milvus Vector Store. Executed in 718.3033623695374 seconds\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "start_time = time.time()\n",
    "vectorstore = Milvus.from_documents(documents=document_chunks, embedding=hf_embeddings,\n",
    "                                    collection_name=\"rafay_ragathon_2024\",\n",
    "                                    connection_args={\"host\": \"milvus\", \"port\": \"19530\"})\n",
    "print(f\"Computing the embeddings for each chunk and loading it to Milvus Vector Store. Executed in {time.time() - start_time} seconds\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9c267-17aa-47d8-b6a5-eb069a69ea2d",
   "metadata": {},
   "source": [
    "## Large Language Model Integration with Triton Inference Server\n",
    "\n",
    "This code snippet is focused on integrating a large language model (LLM) with the Triton Inference Server, specifically utilizing NVIDIA's Triton TensorRT for optimized inference. The script begins by importing necessary components from the langchain and langchain_nvidia_trt libraries, which facilitate the creation of prompt templates and the deployment of language models on Triton, respectively.\n",
    "\n",
    "A server URL and payload (pload) are defined, indicating the address of the Triton server and the configuration for the large language model, including the token limit (500), server URL, and the model name (`ensemble`). This setup implies that an ensemble of models might be used for inference, leveraging Triton's capabilities for managing and scaling AI models.\n",
    "\n",
    "The TritonTensorRTLLM object is initialized with the provided payload, establishing a connection to the Triton server for executing the language model. Furthermore, a detailed prompt template (LLAMA_PROMPT_TEMPLATE) is crafted. This template instructs the model to use provided context and questions to generate helpful answers, emphasizing that the model should refrain from making up answers if uncertain. The template encapsulates instructions, context, and questions within specific formatting tags to guide the model's response generation.\n",
    "\n",
    "Lastly, the PromptTemplate.from_template method is utilized to convert the string-based prompt template into a PromptTemplate object, ready for use with the initialized LLM. This configuration allows for dynamic interaction with the language model, facilitating customized and controlled generation of responses based on the input context and questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d25bc7-1a80-4766-b4e3-691af8560b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_nvidia_trt.llms import TritonTensorRTLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "552c7f36-3d27-4334-ad02-69945ffa9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_url = \"llm:8001\"\n",
    "pload = {\n",
    "            'tokens':500,\n",
    "            'server_url': triton_url,\n",
    "            'model_name': \"ensemble\"\n",
    "}\n",
    "llm = TritonTensorRTLLM(**pload)\n",
    "\n",
    "\n",
    "LLAMA_PROMPT_TEMPLATE = (\n",
    " \"<s>[INST] <<SYS>>\"\n",
    " \"Use the following context to answer the user's question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    " \"<</SYS>>\"\n",
    " \"<s>[INST] Context: {context} Question: {question} Only return the helpful answer below and nothing else. Helpful answer:[/INST]\"\n",
    ")\n",
    "\n",
    "llama_prompt = PromptTemplate.from_template(LLAMA_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c05b4f-63b7-4027-aae2-95359c0f6604",
   "metadata": {},
   "source": [
    "\n",
    "## Question Answering Pipeline with LLM and Data Retrieval\n",
    "\n",
    "This code segment constructs and executes a question answering pipeline using a large language model (LLM) integrated with a data retrieval system. The pipeline is defined as a sequence of operations starting with a data retriever (retriever), which is responsible for fetching relevant context for the provided question. The RunnablePassthrough() function serves as a placeholder, ensuring the question passes through the pipeline unchanged.\n",
    "\n",
    "The operation then proceeds to apply a predefined prompt (LLAMA_PROMPT) to format the question and context appropriately for the LLM. Subsequently, the formatted input is processed by the LLM (llm), which generates a response based on the provided context and the question.\n",
    "\n",
    "The execution timing starts just before the question processing begins. The pipeline utilizes a streaming approach to handle the question, progressively appending each token generated by the LLM to the output string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a8d2d4c-c7fb-4e78-abb8-c4e34a088fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question processed and answered in 0.6793711185455322 seconds\n",
      "Output:\n",
      "  Based on the provided documents, the total revenue of Apple in 2023 is $166,777 million.\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llama_prompt\n",
    "    | llm\n",
    ")\n",
    "start_time = time.time()\n",
    "output = \"\"\n",
    "question = \"What is the total revenue of Apple in 2023?\"\n",
    "for token in chain.stream(question):\n",
    "    output += token\n",
    "    \n",
    "print(f\"Question processed and answered in {time.time() - start_time} seconds\")\n",
    "print(\"Output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c73a2-86b2-4496-8803-27a6bef1ee74",
   "metadata": {},
   "source": [
    "## Evaluating Question Answering Performance with RAGAS Metrics\n",
    "\n",
    "This code segment is designed to evaluate the performance of a question answering system using a dataset of questions and answers. It employs the RAGAS framework to assess the system's ability to provide relevant, faithful, and precise answers based on the retrieved context. Initially, it imports necessary libraries and modules, including pandas for data manipulation and specific metrics from the RAGAS package for evaluation.\n",
    "\n",
    "**The process starts by loading a CSV file containing question and answer pairs, filtering the dataset for entries where the source chunk type is text and the question type aligns with a \"Single-Doc Multi-Chunk RAG\" scenario.** It then initializes a retriever from a vector store to fetch relevant documents based on the questions.\n",
    "\n",
    "For each question in the dataset, the script retrieves contextual information, runs the question through a predefined question answering pipeline (chain), and captures both the generated answer and the inference time. This information is aggregated into a new DataFrame, which is then converted into a Dataset object suitable for evaluation with the RAGAS framework.\n",
    "\n",
    "The evaluation process employs metrics such as context precision, faithfulness, answer relevancy, and context recall, providing a comprehensive assessment of the system's performance across these dimensions. The results of these metrics are averaged to give a general idea of the system's overall effectiveness in generating accurate and relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78822c95-051d-4e6e-8fe0-131765d9a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b831381-9aa3-4369-818f-ce4ca09876e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_data = pd.read_csv(\"./qna_data.csv\")\n",
    "qna_data = qna_data[qna_data[\"Source Chunk Type\"] == \"Text\"]\n",
    "\n",
    "\n",
    "# This single doc multi-chunk RAG is used for sample notebook only. For Hackathon, please use single doc and multiple documents.\n",
    "qna_data = qna_data[qna_data[\"Question Type\"] == \"Single-Doc Multi-Chunk RAG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68b8454e-0763-4845-9b73-7bec2278bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "_data = []\n",
    "for idx, row in qna_data.iterrows():\n",
    "\n",
    "    question = row.iloc[0]\n",
    "    answer =  row.iloc[4]\n",
    "\n",
    "    context = []\n",
    "    _docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    for _doc in _docs:\n",
    "        context.append(_doc.page_content)\n",
    "\n",
    "    # context = \"\\n\".join(context)\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        output = \"\"\n",
    "        for token in chain.stream(question):\n",
    "            output += token\n",
    "            \n",
    "        inference_time = time.time() - start_time\n",
    "    \n",
    "        _data.append([question, answer, context, output, inference_time])\n",
    "    except Exception as ex:\n",
    "        print(\"Error\", ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "681a8acf-06a7-4547-8ffe-cf38132171f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 112/112 [00:30<00:00,  3.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "context_precision    0.754960\n",
       "faithfulness         0.479379\n",
       "answer_relevancy     0.735310\n",
       "context_recall       0.633929\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval = pd.DataFrame(_data, columns=[\"question\", \"ground_truth\", \"contexts\", \"answer\", \"inference_time\"])\n",
    "eval_dataset = Dataset.from_pandas(df_eval)\n",
    "result = evaluate(eval_dataset, metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],)\n",
    "res = result.to_pandas()[[\"context_precision\",\"faithfulness\",\"answer_relevancy\",\"context_recall\"]]\n",
    "res.fillna(0.0,inplace=True)\n",
    "res.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d735306-7791-4903-a255-d67c257c8abe",
   "metadata": {},
   "source": [
    "## Judging Criteria\n",
    "RAG-a-Thon judging will be performed by Rafay & NVIDIA and will be based on 1) the performance and 2) the production readiness of your RAG application. Model performance depends on your underlying RAG strategy, which is typically a combination of data chunking, embedding, retrieval, and text generation models. The performance will be assessed using [Ragas open source library](https://github.com/explodinggradients/ragas) for evaluating RAG. Production readiness will be judged based on the top considerations described here, including: \n",
    "\n",
    "## Submission.\n",
    "\n",
    "To submit your RAG application’s performance, perform the following two actions: \n",
    "1. Take a screenshot of your metrics and post it on both Linkedin and X using the hashtag #RafayGTCRagaThon\n",
    "2. Lastly, email the metrics along with the Jupyter Notebook with your implementation to [ragathon@rafay.co](mailto:ragathon@rafay.co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb856b-e74c-48ce-9f07-ac12f26ad1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
